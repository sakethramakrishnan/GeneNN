{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import GINDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.data\n",
    "from torch.utils.data import DataLoader\n",
    "from pydantic import BaseModel\n",
    "from Bio.SeqUtils import GC\n",
    "from typing import Any, Dict, List, Optional, Set, Type, Union, Tuple\n",
    "from pathlib import Path\n",
    "PathLike = Union[str, Path]\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "import random\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM, AutoConfig, BatchEncoding\n",
    "from transformers.utils import ModelOutput\n",
    "from tokenizers import Tokenizer\n",
    "#from genslm import GenSLM, SequenceDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenSLM(nn.Module):\n",
    "\n",
    "    __genslm_path = Path(\"/home/couchbucks/Documents/saketh/GeneNN/genslm/genslm\")\n",
    "    __tokenizer_path = __genslm_path / \"tokenizer_files\"\n",
    "    __architecture_path = __genslm_path / \"architectures\"\n",
    "\n",
    "    MODELS: Dict[str, Dict[str, str]] = {\n",
    "        \"genslm_25M_patric\": {\n",
    "            \"config\": str(__architecture_path / \"neox\" / \"neox_25,290,752.json\"),\n",
    "            \"tokenizer\": str(__tokenizer_path / \"codon_wordlevel_69vocab.json\"),\n",
    "            \"weights\": \"patric_25m_epoch01-val_loss_0.57_bias_removed.pt\",\n",
    "            \"seq_length\": \"2048\",\n",
    "        },\n",
    "        \"genslm_250M_patric\": {\n",
    "            \"config\": str(__architecture_path / \"neox\" / \"neox_244,464,576.json\"),\n",
    "            \"tokenizer\": str(__tokenizer_path / \"codon_wordlevel_69vocab.json\"),\n",
    "            \"weights\": \"patric_250m_epoch00_val_loss_0.48_attention_removed.pt\",\n",
    "            \"seq_length\": \"2048\",\n",
    "        },\n",
    "        \"genslm_2.5B_patric\": {\n",
    "            \"config\": str(__architecture_path / \"neox\" / \"neox_2,533,931,008.json\"),\n",
    "            \"tokenizer\": str(__tokenizer_path / \"codon_wordlevel_69vocab.json\"),\n",
    "            \"weights\": \"patric_2.5b_epoch00_val_los_0.29_bias_removed.pt\",\n",
    "            \"seq_length\": \"2048\",\n",
    "        },\n",
    "        \"genslm_25B_patric\": {\n",
    "            \"config\": str(__architecture_path / \"neox\" / \"neox_25,076,188,032.json\"),\n",
    "            \"tokenizer\": str(__tokenizer_path / \"codon_wordlevel_69vocab.json\"),\n",
    "            \"weights\": \"model-epoch00-val_loss0.70-v2.pt\",\n",
    "            \"seq_length\": \"2048\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_id: str, model_cache_dir: PathLike = \".\") -> None:\n",
    "        \"\"\"GenSLM inference module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_id : str\n",
    "            A model ID corresponding to a pre-trained model. (e.g., genslm_25M_patric)\n",
    "        model_cache_dir : PathLike, optional\n",
    "            Directory where model weights have been downloaded to (defaults to current\n",
    "            working directory). If model weights are not found, then they will be\n",
    "            downloaded, by default \".\"\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If model_id is invalid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_cache_dir = Path(model_cache_dir)\n",
    "        self.model_info = self.MODELS.get(model_id)\n",
    "        if self.model_info is None:\n",
    "            valid_model_ids = list(self.MODELS.keys())\n",
    "            raise ValueError(\n",
    "                f\"Invalid model_id: {model_id}. Please select one of {valid_model_ids}\"\n",
    "            )\n",
    "\n",
    "        self._tokenizer = self.configure_tokenizer()\n",
    "        self.model = self.configure_model()\n",
    "\n",
    "    @property\n",
    "    def seq_length(self) -> int:\n",
    "        assert self.model_info is not None\n",
    "        return int(self.model_info[\"seq_length\"])\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self) -> PreTrainedTokenizerFast:\n",
    "        return self._tokenizer\n",
    "\n",
    "    def configure_model(self) -> AutoModelForCausalLM:\n",
    "        assert self.model_info is not None\n",
    "        base_config = AutoConfig.from_pretrained(self.model_info[\"config\"])\n",
    "        model = AutoModelForCausalLM.from_config(base_config)\n",
    "\n",
    "        weight_path = self.model_cache_dir / self.model_info[\"weights\"]\n",
    "        if not weight_path.exists():\n",
    "            # TODO: Implement model download\n",
    "            raise NotImplementedError\n",
    "        ptl_checkpoint = torch.load(weight_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(ptl_checkpoint[\"state_dict\"], strict=False)\n",
    "        return model\n",
    "\n",
    "    def configure_tokenizer(self) -> PreTrainedTokenizerFast:\n",
    "        assert self.model_info is not None\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object=Tokenizer.from_file(self.model_info[\"tokenizer\"])\n",
    "        )\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        return tokenizer\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, **kwargs: Any\n",
    "    ) -> ModelOutput:\n",
    "        return self.model(\n",
    "            input_ids, labels=input_ids, attention_mask=attention_mask, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):  # type: ignore[type-arg]\n",
    "    \"\"\"Dataset initialized from a list of sequence strings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequences: List[str],\n",
    "        seq_length: int,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        kmer_size: int = 3,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.batch_encodings = self.tokenize_sequences(\n",
    "            sequences, tokenizer, seq_length, kmer_size, verbose\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_sequences(\n",
    "        sequences: List[str],\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        seq_length: int,\n",
    "        kmer_size: int = 3,\n",
    "        verbose: bool = True,\n",
    "    ) -> List[BatchEncoding]:\n",
    "\n",
    "        tokenizer_fn = functools.partial(\n",
    "            tokenizer,\n",
    "            max_length=seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch_encodings = [\n",
    "            tokenizer_fn(SequenceDataset.group_by_kmer(seq, kmer_size))\n",
    "            for seq in tqdm(sequences, desc=\"Tokenizing...\", disable=not verbose)\n",
    "        ]\n",
    "        return batch_encodings\n",
    "\n",
    "    @staticmethod\n",
    "    def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "        return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.batch_encodings)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        batch_encoding = self.batch_encodings[idx]\n",
    "        # Squeeze so that batched tensors end up with (batch_size, seq_length)\n",
    "        # instead of (batch_size, 1, seq_length)\n",
    "        sample = {\n",
    "            \"input_ids\": batch_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": batch_encoding[\"attention_mask\"],\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenSLM(\n",
       "  (model): GPTNeoXForCausalLM(\n",
       "    (gpt_neox): GPTNeoXModel(\n",
       "      (embed_in): Embedding(69, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (act): FastGELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embed_out): Linear(in_features=512, out_features=69, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GenSLM(\"genslm_25M_patric\", model_cache_dir=\"/home/couchbucks/Documents/saketh/GeneNN/GeneNN/\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 1/1 [00:00<00:00, 282.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SequenceDataset object at 0x7f33b6ec0ca0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f33b6ec13c0>\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"ATGAAC\",\n",
    "]\n",
    "\n",
    "dataset = SequenceDataset(sequences, model.seq_length, model.tokenizer)\n",
    "print(dataset)\n",
    "dataloader = DataLoader(dataset)\n",
    "print(dataloader)\n",
    "# Compute averaged-embeddings for each input sequence\n",
    "embeddings = []\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        #print(batch[\"input_ids\"])\n",
    "        outputs = model(\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        attention = outputs.attentions[0]\n",
    "        for x in attention:\n",
    "            for y in x:\n",
    "                for z in y:\n",
    "                    print(z)\n",
    "        \n",
    "        # outputs.hidden_states shape: (layers, batch_size, sequence_length, hidden_size)\n",
    "        # Use the embeddings of the last layer\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdd3115b3b6908f03d8d90c1d9f618771832086b4a59de7f0ce6ece3f382aed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
